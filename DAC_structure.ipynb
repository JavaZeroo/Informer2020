{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.DCdetector import DCdetector\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 1440\n",
    "# patch_size = 1440\n",
    "channel = 2\n",
    "enc_in = 10\n",
    "c_out = 2\n",
    "model = DCdetector(win_size, c_out, c_out, n_heads=1, d_model=256, e_layers=3, patch_size=[3,5,7], channel=12, d_ff=512, dropout=0.0, activation='gelu', output_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing mean-reduction pattern \"(b reduce_b) l m n-> b l m n\".\n Input tensor shape: torch.Size([2, 1, 1440, 1440]). Additional info: {'reduce_b': 12}.\n Shape mismatch, can't divide axis of length 2 in chunks of 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/einops/einops.py:523\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    522\u001b[0m     recipe \u001b[39m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_names\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(axes_lengths), ndim\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(shape))\n\u001b[0;32m--> 523\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_recipe(\n\u001b[1;32m    524\u001b[0m         backend, recipe, cast(Tensor, tensor), reduction_type\u001b[39m=\u001b[39;49mreduction, axes_lengths\u001b[39m=\u001b[39;49mhashable_axes_lengths\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m EinopsError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/einops/einops.py:234\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(backend, recipe, tensor, reduction_type, axes_lengths)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added \u001b[39m=\u001b[39m _reconstruct_from_shape(\n\u001b[1;32m    235\u001b[0m         recipe, backend\u001b[39m.\u001b[39;49mshape(tensor), axes_lengths\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     \u001b[39m# shape or one of passed axes lengths is not hashable (i.e. they are symbols)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/einops/einops.py:187\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape, axes_dims)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(length, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(known_product, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m length \u001b[39m%\u001b[39m known_product \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mraise\u001b[39;00m EinopsError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape mismatch, can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt divide axis of length \u001b[39m\u001b[39m{\u001b[39;00mlength\u001b[39m}\u001b[39;00m\u001b[39m in chunks of \u001b[39m\u001b[39m{\u001b[39;00mknown_product\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m unknown_axis \u001b[39m=\u001b[39m unknown_axes[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mEinopsError\u001b[0m: Shape mismatch, can't divide axis of length 2 in chunks of 12",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/ljb/Informer2020/DAC_structure.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.88.4/home/ljb/Informer2020/DAC_structure.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.88.4/home/ljb/Informer2020/DAC_structure.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# model = model.cpu()\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.88.4/home/ljb/Informer2020/DAC_structure.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model(x\u001b[39m.\u001b[39;49mcuda())\n",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Informer2020/models/DCdetector.py:270\u001b[0m, in \u001b[0;36mDCdetector.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    267\u001b[0m     x_patch_num \u001b[39m=\u001b[39m rearrange(x_patch_num, \u001b[39m'\u001b[39m\u001b[39mb m (p n) -> (b m) p n\u001b[39m\u001b[39m'\u001b[39m, p \u001b[39m=\u001b[39m patchsize) \n\u001b[1;32m    268\u001b[0m     x_patch_num \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_patch_num[patch_index](x_patch_num)\n\u001b[0;32m--> 270\u001b[0m     series, prior \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x_patch_size, x_patch_num, x_ori, patch_index)\n\u001b[1;32m    271\u001b[0m     series_patch_mean\u001b[39m.\u001b[39mappend(series), prior_patch_mean\u001b[39m.\u001b[39mappend(prior)\n\u001b[1;32m    273\u001b[0m series_patch_mean \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_flatten(series_patch_mean))\n",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Informer2020/models/DCdetector.py:212\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x_patch_size, x_patch_num, x_ori, patch_index, attn_mask)\u001b[0m\n\u001b[1;32m    210\u001b[0m prior_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    211\u001b[0m \u001b[39mfor\u001b[39;00m attn_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_layers:\n\u001b[0;32m--> 212\u001b[0m     series, prior \u001b[39m=\u001b[39m attn_layer(x_patch_size, x_patch_num, x_ori, patch_index, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m    213\u001b[0m     series_list\u001b[39m.\u001b[39mappend(series)\n\u001b[1;32m    214\u001b[0m     prior_list\u001b[39m.\u001b[39mappend(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Informer2020/models/DCdetector.py:88\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[0;34m(self, x_patch_size, x_patch_num, x_ori, patch_index, attn_mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m B, L, _ \u001b[39m=\u001b[39m x_ori\u001b[39m.\u001b[39mshape\n\u001b[1;32m     86\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_projection(x_ori)\u001b[39m.\u001b[39mview(B, L, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m series, prior \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_attention(\n\u001b[1;32m     89\u001b[0m     queries_patch_size, queries_patch_num,\n\u001b[1;32m     90\u001b[0m     keys_patch_size, keys_patch_num,\n\u001b[1;32m     91\u001b[0m     values, patch_index,\n\u001b[1;32m     92\u001b[0m     attn_mask\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m series, prior\n",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Informer2020/models/DCdetector.py:40\u001b[0m, in \u001b[0;36mDAC_structure.forward\u001b[0;34m(self, queries_patch_size, queries_patch_num, keys_patch_size, keys_patch_num, values, patch_index, attn_mask)\u001b[0m\n\u001b[1;32m     38\u001b[0m series_patch_size \u001b[39m=\u001b[39m repeat(series_patch_size, \u001b[39m'\u001b[39m\u001b[39mb l m n -> b l (m repeat_m) (n repeat_n)\u001b[39m\u001b[39m'\u001b[39m, repeat_m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size[patch_index], repeat_n\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size[patch_index])    \n\u001b[1;32m     39\u001b[0m series_patch_num \u001b[39m=\u001b[39m series_patch_num\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size[patch_index],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_size[patch_index]) \n\u001b[0;32m---> 40\u001b[0m series_patch_size \u001b[39m=\u001b[39m reduce(series_patch_size, \u001b[39m'\u001b[39;49m\u001b[39m(b reduce_b) l m n-> b l m n\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m, reduce_b\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchannel)\n\u001b[1;32m     41\u001b[0m series_patch_num \u001b[39m=\u001b[39m reduce(series_patch_num, \u001b[39m'\u001b[39m\u001b[39m(b reduce_b) l m n-> b l m n\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, reduce_b\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_attention:\n",
      "File \u001b[0;32m~/miniconda3/envs/sde/lib/python3.11/site-packages/einops/einops.py:533\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    531\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Input is list. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAdditional info: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 533\u001b[0m \u001b[39mraise\u001b[39;00m EinopsError(message \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing mean-reduction pattern \"(b reduce_b) l m n-> b l m n\".\n Input tensor shape: torch.Size([2, 1, 1440, 1440]). Additional info: {'reduce_b': 12}.\n Shape mismatch, can't divide axis of length 2 in chunks of 12"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, win_size, c_out)\n",
    "model = model.cuda()\n",
    "# model = model.cpu()\n",
    "model(x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
